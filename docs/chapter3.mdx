---
sidebar_position: 4
---

import PersonalizedTooltip from '@site/src/components/PersonalizedTooltip';

# Chapter 3: AI Models for Physical Systems

The intelligence that drives <PersonalizedTooltip term="Physical AI">Physical AI</PersonalizedTooltip> comes from <PersonalizedTooltip term="machine learning">machine learning</PersonalizedTooltip> models trained to perceive, plan, and act in the physical world. This chapter explores the major paradigms for robot learning, from <PersonalizedTooltip term="reinforcement learning">reinforcement learning</PersonalizedTooltip> to <PersonalizedTooltip term="foundation model">foundation models</PersonalizedTooltip>.

## Reinforcement Learning for Robotics

<PersonalizedTooltip term="reinforcement learning">Reinforcement Learning (RL)</PersonalizedTooltip> enables robots to discover optimal behaviors through trial and error, learning from the consequences of their actions rather than explicit instruction.

### Core Concepts

RL frames robot learning as optimizing cumulative reward:

- **Agent**: The robot acting in the environment
- **State**: Current configuration and observations
- **Action**: Commands sent to <PersonalizedTooltip term="actuator">actuators</PersonalizedTooltip>
- **Reward**: Scalar feedback indicating desirability of outcomes
- **Policy**: Mapping from states to actions that the agent learns

The goal is learning a policy that maximizes expected cumulative reward over time.

### Deep Reinforcement Learning

<PersonalizedTooltip term="deep learning">Deep neural networks</PersonalizedTooltip> enable RL to scale to high-dimensional observations and complex behaviors:

**<PersonalizedTooltip term="DQN">Deep Q-Networks (DQN)</PersonalizedTooltip>** learn to estimate the value of taking each action in each state. This approach works for discrete action spaces.

**Policy Gradient Methods** directly optimize the policy parameters using gradient ascent on expected reward. REINFORCE is the canonical algorithm.

**Actor-Critic Methods** combine value estimation (critic) with direct policy optimization (actor):

- **A3C** (Asynchronous Advantage Actor-Critic): Multiple agents explore in parallel
- **<PersonalizedTooltip term="PPO">PPO</PersonalizedTooltip>** (Proximal Policy Optimization): Constrains policy updates for stable learning
- **<PersonalizedTooltip term="SAC">SAC</PersonalizedTooltip>** (Soft Actor-Critic): Maximizes entropy alongside reward for exploration
- **TD3** (Twin Delayed DDPG): Improves continuous control through careful algorithm design

These algorithms have achieved impressive results in simulation and some real-world tasks.

### Challenges in Robot RL

Despite successes, RL faces significant obstacles for Physical AI:

**Sample Inefficiency**: RL typically requires millions of environment interactions to learn. Real robots cannot afford this much experimentation - it would take years and risk damage.

**Exploration vs. Exploitation**: Balancing trying new actions (exploration) with using known good actions (exploitation) is particularly challenging when exploration might break the robot.

**Reward Engineering**: Designing reward functions that lead to desired behaviors without unintended consequences is difficult. Small specification errors can lead to unexpected outcomes.

**<PersonalizedTooltip term="sim-to-real">Sim-to-Real Gap</PersonalizedTooltip>**: Training in simulation is much faster, but behaviors don't always transfer to the real world due to modeling inaccuracies.

**Safety**: RL agents may explore dangerous states during learning, risking damage or injury.

Research continues on making RL more practical for robotics through demonstrations, simulation, and improved algorithms.

## Imitation Learning and Teleoperation

Rather than learning from scratch through RL, robots can learn by imitating expert demonstrations. This dramatically improves sample efficiency.

### Behavioral Cloning

The simplest approach is supervised learning: collect expert demonstrations and train a policy to predict expert actions given observed states.

**Process**:
1. Human demonstrates task multiple times
2. Record state-action pairs
3. Train <PersonalizedTooltip term="neural network">neural network</PersonalizedTooltip> to map states to actions
4. Deploy learned policy on robot

**Advantages**:
- Sample efficient - learns from relatively few demonstrations
- No reward engineering needed
- Leverages human expertise

**Limitations**:
- Distribution shift: Errors compound as policy encounters states not in training data
- Cannot improve beyond demonstrator
- Struggles with multi-modal action distributions

### DAgger (Dataset Aggregation)

DAgger addresses distribution shift by iteratively:
1. Deploy current policy
2. Expert provides corrections when policy makes mistakes
3. Add expert corrections to training data
4. Retrain policy

This allows the expert to demonstrate recovery from the mistakes the learned policy makes.

### Inverse Reinforcement Learning

Rather than imitating actions, <PersonalizedTooltip term="inverse reinforcement learning">IRL</PersonalizedTooltip> infers the reward function the expert is optimizing. This recovered reward can then train RL agents or evaluate new behaviors.

**MaxEnt IRL** finds reward functions that make expert demonstrations most likely while maximizing policy entropy. This prefers simpler explanations.

**<PersonalizedTooltip term="GAIL">Generative Adversarial Imitation Learning (GAIL)</PersonalizedTooltip>** frames IRL as a GAN problem where the generator tries to match expert behavior distribution.

IRL is computationally expensive but can enable generalization beyond demonstrated scenarios.

### Teleoperation Systems

<PersonalizedTooltip term="teleoperation">Teleoperation</PersonalizedTooltip> allows humans to control robots remotely, simultaneously enabling task execution and data collection:

**Virtual Reality (VR) Teleoperation** provides immersive control with natural hand movements mapped to robot actions. Companies like Sanctuary AI use VR extensively for data collection.

**Motion Capture** tracks human motion and retargets it to robot kinematics. Useful for collecting demonstrations of manipulation tasks.

**Haptic Feedback** provides force feedback to the operator, enabling delicate manipulation and improving task performance.

Modern teleoperation systems collect thousands of hours of demonstration data used to train autonomous policies.

## Foundation Models for Robotics

Large models trained on internet-scale data are revolutionizing robotics by providing general capabilities and world knowledge.

### Vision-Language Models

Models like GPT-4V, Gemini, and Claude understand both images and text, enabling:

**Visual Question Answering**: "What objects are on the table?"
**Instruction Grounding**: "Pick up the red cup" → Identify red cup in image
**Common-sense Reasoning**: Understanding typical object properties and uses
**Task Decomposition**: Breaking high-level goals into sub-tasks

These capabilities make robots more accessible to non-expert users through natural language interfaces.

### Robot-Specific Foundation Models

Several projects train large models specifically for robotics:

**<PersonalizedTooltip term="RT-1">RT-1</PersonalizedTooltip>** (Robotics Transformer 1) from Google trains a vision-language-action model on diverse robot tasks. The model takes image observations and language instructions, outputting robot actions. Training on 130k demonstrations across 700+ tasks enables generalization to new scenarios.

**<PersonalizedTooltip term="RT-2">RT-2</PersonalizedTooltip>** extends this by fine-tuning web-scale vision-language models (PaLI-X) for robotics. This grounds internet knowledge in robot actions, dramatically improving reasoning and generalization. RT-2 can follow instructions requiring world knowledge that was never demonstrated.

**PaLM-E** integrates robot sensor data directly into the pre-training of large language models. This creates a true multi-modal foundation model that reasons about language, vision, and robotics jointly.

**<PersonalizedTooltip term="Octo">Octo</PersonalizedTooltip>** is an open-source generalist robot policy trained on 800k trajectories from multiple robot platforms. It provides a foundation that can be fine-tuned for specific robots and tasks.

### Benefits of Foundation Models

- **Sample Efficiency**: Pre-training on large datasets provides a strong starting point
- **<PersonalizedTooltip term="transfer learning">Transfer Learning</PersonalizedTooltip>**: Knowledge generalizes across tasks and embodiments
- **Common-sense Reasoning**: Models bring world knowledge to robotics
- **Language Interface**: Natural language control becomes possible
- **Rapid Adaptation**: Fine-tuning adapts to new robots/tasks quickly

### Limitations and Challenges

- **Hallucination**: Models may confidently generate incorrect outputs
- **Grounding**: Ensuring model outputs correspond to physical reality
- **Safety**: Large models can be unpredictable in novel situations
- **Compute Requirements**: Running large models requires significant hardware
- **Data Efficiency**: While better than RL, still requires substantial data

Foundation models represent a paradigm shift toward more general and capable Physical AI systems.

## Sim-to-Real Transfer Learning

Training in <PersonalizedTooltip term="simulation">simulation</PersonalizedTooltip> is much faster than the real world, but simulation cannot perfectly capture physical reality. Bridging this "reality gap" is crucial for practical Physical AI.

### The Reality Gap

Simulations differ from reality in multiple ways:

**Physics Modeling**: Contact dynamics, friction, and deformation are approximated
**Sensor Modeling**: Noise, artifacts, and failure modes differ
**Visual Appearance**: Lighting, materials, and textures are simplified
**Actuation**: Motor response, backlash, and compliance are idealized
**Latency**: Simulation typically has zero latency while real systems have delays

These differences mean policies trained in simulation often fail when deployed on real robots.

### Domain Randomization

The key insight: if a policy works across many different simulated environments, it's more likely to work in the real world.

**Visual Domain Randomization** randomizes:
- Lighting conditions
- Object textures and colors
- Camera parameters
- Background scenes

**Dynamics Domain Randomization** varies:
- Object masses and friction
- Motor gains and delays
- Link lengths and inertias
- External disturbances

**Adversarial Domain Randomization** actively searches for challenging parameter settings during training.

By training on diverse simulated environments, policies become robust to the inevitable mismatch between simulation and reality.

### System Identification

Rather than hoping randomization covers reality, system identification measures real-world parameters and tunes simulation to match:

**Trajectory Optimization** fits simulator parameters to make predicted trajectories match observed real-world behavior.

**Bayesian Optimization** efficiently searches parameter space to minimize sim-to-real difference.

**Neural Network Dynamics Models** learn corrections to physics simulation from real data.

Accurate simulation reduces the reality gap, making transfer more reliable.

### Residual Learning

Train two policies:
1. **Base Policy** trained in simulation
2. **Residual Policy** learned in real world that outputs corrections

The final action is: `action = base_policy(state) + residual_policy(state)`

This allows leveraging simulation while adapting to real-world specifics without discarding simulation training.

### Reality-Centric Approaches

Some recent work inverts the traditional sim-to-real pipeline:

**<PersonalizedTooltip term="NeRF">Neural Radiance Fields (NeRFs)</PersonalizedTooltip>** and **Gaussian Splatting** create photorealistic 3D scenes from multi-view images. These can serve as training environments that precisely match reality.

**<PersonalizedTooltip term="digital twin">Digital Twins</PersonalizedTooltip>** that continuously update from real-world sensors provide increasingly accurate simulation.

**Real-to-Sim-to-Real** uses reality capture to create accurate simulation, trains there, and transfers back.

### Hybrid Approaches

Combining simulation and real-world experience yields best results:

- Pre-train in simulation for sample efficiency
- Fine-tune on real robot for accuracy
- Use simulation for exploration, real world for validation
- Alternate between sim and real training

Most successful real-world deployments employ such hybrid strategies.

## Emerging Paradigms

Several newer approaches show promise for Physical AI:

### Self-Supervised Learning

Learning representations without manual labels:
- **Contrastive Learning**: Similar observations should have similar representations
- **Predictive Models**: Predict future states from current observations
- **Autoencoders**: Compress and reconstruct sensory data

Self-supervised learning enables robots to discover useful features from interaction.

### Meta-Learning

Learning to learn - training systems that can quickly adapt to new tasks:
- **<PersonalizedTooltip term="MAML">MAML</PersonalizedTooltip>** (Model-Agnostic Meta-Learning): Fine-tunes in few steps
- **Few-Shot Learning**: Learns from minimal examples
- **Task-Conditional Policies**: Single network handles multiple tasks

Meta-learning promises robots that rapidly adapt to new environments and requirements.

### Offline Reinforcement Learning

Learning from datasets of past experience without online interaction:
- Enables learning from suboptimal demonstrations
- Uses previously collected data efficiently
- Avoids dangerous exploration

This makes RL more practical by leveraging logged data.

## Conclusion

AI models for Physical systems span multiple paradigms, each with strengths and limitations. Reinforcement learning enables autonomous discovery of behaviors but requires massive data. Imitation learning leverages human expertise but is limited by demonstrations. Foundation models bring internet-scale knowledge but require careful grounding.

The future likely involves combining these approaches - using foundation models for high-level reasoning, imitation learning for skill acquisition, and RL for fine-tuning. As models improve and data accumulates, Physical AI systems will become more capable, sample-efficient, and practical.

---

**Next Chapter**: [The Rise of Humanoid Robots →](/docs/chapter4)
